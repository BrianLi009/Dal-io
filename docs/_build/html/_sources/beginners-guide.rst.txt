.. _beginners_guide:

Understanding Graphs
====================

What do I mean by "graphical structure"?
----------------------------------------

In a graphical structures data is represented as nodes and operations as edges. Think of it as a way to represent many inter-connected transformations and their input and output data.

Progressive Disclosure of Complexity
------------------------------------

The main philosophy begind the graphical structure of Dal-io come from the Deep Learning library Keras. In their documentation, they state that "A core principle of Keras is **progressive disclosure of complexity**. You should always be able to get into lower-level workflows in a gradual way. You shouldn't fall off a cliff if the high-level functionality doesn't exactly match your use case. You should be able to gain more control over the small details while retaing a commensurate amount of high-level convenience."

So you are familiar with Keras, you will understand that they provide users with a plethora of pre-implemented classes (layers and models) that fit into each other, though the user is also free to create subclasses of their own that can be integrated into the Deep Neural Network and iteract with it as just another layer.

Likewise, all of the classes described below where made with the objective of being easily customized by more experienced users. After all, the great majority of objects you will be using where implemented like that! Once you feel like you got a hang of Dal-io and want to build your own pieces, check out the `source code <https://github.com/renatomatz/Dal-io>`_ or the :ref:`developers_guide`.


Why is a graphical structure optimal for financial modeling?
------------------------------------------------------------

    * Modern automated finantial models retrieve data, clean and dirty, from various sources and through cleaning and itegration are able to join them, further process this product and finally derive insigts. The problem is that as these models utilize more and more data from various sources, created models tend to become confusing for both technical and non technical people. Also, as there is no unified workflow to deal with these, created models tend to become highly inflexible and lacking portability (onto other models or projects.) A graphical architecture offers an intuitive workflow for working with data, where inputs can have a unified translation, data can be constantly checked for validity and otuputs can be used in flexible ways as parts of a bigger system or drive actions.

    * Utilizing large ammounts of data can also end up being highly memory-inneficient when data sources are varied and outputs are as simple as a buy/sell command. As in the tensorflow graphical architecture, using these constructs allow for automatic parallelization of models to better use modern hardware. Applications can also be built to fit multiple models, and updated independently from the rest of the system.

    * Graphs are easy to interpret visualy, which is useful for understanding the flow of data and interpreting output or bugs. They are also higly flexible, allowing users to modify pieces or generate new connections while keeping an enforcable system of data integrity.

    * Perhaps most importantly, these graphs are extremely lightweight and portable, which is key for widespread distribution and access. While every piece can be accessed and tested on-the-go for better ease of development, they are ultimately just pieces of a bigger structure, where data flows continuously and leftover data is dicared automatically, keeping the memory and processing burden at a minimum when dealing with massive datasets.


Base Classes
============

These are the classes you will use throughout an analalysis, or rather a class that implements their functionality. Getting to know them is important as it makes it easier to identify one when you see one and make it easier to search for one when you don't really remember where to find it.

.. _external:

External <_Node>
----------------

**Manage connections between your environment and an external source.**

Every model requires an origin to the data it uses, and often wants to send this data out again once it's processed. Subclasses of :code:`External` will implement systems to manage the input and output of data to and from an external sources. An external source is any data or application located outside of your python environment. Two common examples are files and graphs. While these can be manipulated from the python environemt, the actual data is stored outside.

:code:`External` class instacnes will often be redundant with existing connection handlers, but at least subclasses will allow for more integrated connection handling and collection, so that you can have a single supplicant object for each external connection.

As a child class of :code:`_Node`,:code:`External` implements the :code:`.request(**kwargs)` method, which takes in requests and executed valid ones on their external connections.

While this method is responsible for the main requests to and from the data, subclasses will often have other methods to perform more specific actions on it. Additionally, the :code:`**kwargs` parameter will rarely be the same as the one relayed through the :code:`_Transformer.run()` as  :code:`Translator` and :code:`Application` instances will often curate these to be more generalizable to multiple :code:`External` implementations.

**What to Look For:**

    #. What the external source is.

    #. Is it reliant on configuration? If so, what configuration parameters are required/considered?


_Transformer
------------

**Represent data transformations.**

:code:`_Transformer` instances are defined by their inputs and outputs. IO can be limmited to one or more sources and the source can be either internal or external (as defined in :ref:`external`). 

All :code:`_Transformer` instances implement the the :code:`.run(**kwargs)` method to:

    #. Request source data from a :code:`_Node` instance.

    #. Apply specific transformations to the sourced data.

    #. Return the transformed data.

This process will vary depending on the subclass, though the one thing to keep in mind is that the output of this method is what will be fed onto the next node on the graph, so it's a powerful tool for debugging.

:code:`_Transformer` instances also define each input in their initialization by using :code:`Validator` instances. You can find more about these in the Developpers Guide section on the :ref:`validator` but for now, you can use the :code:`_Transformer.describe()` method to get an idea of what kind of inputs this piece requires or prefers.

.. Talk about the input methods and copy

You won't be using these directly in your analyses, but will definitely use one of its subclasses.

**What to Look For:**

    #. Number of input and outputs.

    #. Sources/destinations of inputs and outputs.

    #. Input descriptions.

Translator <_Transformer>
-------------------------

**Request and standardize external data.**

*One external input, one internal output*

While :code:`External` instances are the origin of all data, :code:`Translator` instances are the root of all *clean and standardized* data. Objects of this class have :code:`External` instances as their source and are tasked with creating requests undestandable by that instance and standardize the response data into a useable format. 

For more information on the Dal-io formatting standards, check out :ref:`formatting`.

All :code:`Translator` instances implement the the :code:`.run(**kwargs)` method to:

    #. Source data from an :code:`External` instance.

    #. Translate the data into a format as specified by the formatting guide.

    #. Return the translated data.

These also tend to be the PipeLine stages where :code:`kwargs` source from.

**What to Look For:**

    #. Compatible :code:`External` instances.

    #. What translation format is being used and how will the output contain.

    #. What are the keyword arguments it can interpret.

Pipe <_Transformer>
-------------------

**Transform a single input into a single output.**

*One internal input, one internal output*

Pipes will compose the majority of data wranging and processing in your graphs, and are designed to be easily extendable by users.

All pipes must implement the :code:`.transform(data, **kwargs)` method, which takes in the output from sourced data and returns it transformed. This has three main purposes.

    #. Subclasses can more objectively focuss on transforming and outputting the :code:`data` parameter instead of having to deal with sourcing it.

    #. It makes it possible to use :code:`Pipe` instances to transform data outside of the Dal-io library directly, which is useful for applications outside of the library's scope or for testing the transformation.

    #. More efficient compatibility with :ref:`pipeline` objects.

All :code:`Pipe` instances implement the the :code:`.run(**kwargs)` method to:

    #. Define input requirements.

    #. Source data from another :code:`_Transformer` instance, applying integrity checks.

    #. Pass it as the :code:`data` parameter to the :code:`.transform()` method.
    #. Return the transformed data.

While the default implementation of the :code:`.run()` method simply sources data and passes into :code:`.transform`, it is often changed to modify keyword arguments passed onto the source node and the .transform() call. 

**What to Look For:**

    #. What are the input requirements.

    #. What the :code:`.transform` method does.

    #. What are changeable attributes that affect the data processing.

Model <_Transformer>
--------------------

Models are a lot like transformers as they take in inputs and has a single output. Models do differ from transformers as they can take in multiple inputs and be much more flexible with additional methods for different strategies or for small data storage. Also, keep in mind models do not have a \_\_call\_\_ method inherited or a single function that transforms its inputs. Models are supposed to perform more intricate operations, beyond a simple transformation.

**What to Look For:**

    #. 

Applications <Model>
--------------------

While Models are normally the last stage of a model, it still has a single output which might have limited value in itself. Applications are tools used for the interpretation of multiple, which are not constrained by those output by models, but often are. These can have a broad range of applications, from graphing to trading. The main functionality is in the .execute() method, which gets input data and interprets it as needed. 

**What to Look For:**

    #. 

Extra Classes and Concepts
==========================

Now that we've seen what will make your models work, lets jump into what will make your models **work incredibly.** 

.. _pipeline:

PipeLine <Pipe>
---------------

As Pipe instances implement a normally small operation and have only one imput and one output, you are able to join them together, thorugh the \_\_add\_\_() internal method (which overrides the + operator) to create a sequence of transformations linked one after the other. These simply pass the output of one Pipe instance's .transform() method as the input to another, which can be a significant speed boost, though you should be carefull with data integrity here. 

KEEP IN MIND that good alternatives to these is just linking Pipe instances together in order to validate the data at every stage of the pipeline. This will have the same output as a PipeLine, but compromise on speed and possibly aesthetics.

Memory <_Transformer>
---------------------

When using APIs to fetch online data, there is often a delay that ranges from a few to a few dozen seconds. This might be completely fine if data will only pass through your model once to feed an application, for example, but will become a problem if you are also performing analyses on several pieces of the model or have several Model instances in your graph (which call on an input once for every source). The solution to this lies in Memory instances that temporarily save model imputs to some location and retrieves it when ran. 

Notice that Memory inherits from a _Transformer, which makes it compatible as input to any piece of your graph and behaves like any other input (most closely resembling a Pipe.)

Subclasses will implement different storage strategies for different locations. These will have their own data requirements and storage and retrieval logic - imagine the different in data structure, storage and retrieval required for storing data on a database vs on the local python session.

One thing to keep in mind is that these only store one piece of memory, so if you, for example, want to vary your .run() kwargs, this might not be the best option beyond building and debugging your model. If you still want the speed advantages of Memory while allowing for more runtime argument flexibility, check out the LazyRunner class below.

LazyRunner <_Transformer>
-------------------------

These objects are the solution to storing multible Memory instances for different runtime kwargs that pass through the instance. These do not store the data itself, but rather the memory instances that do. This allows for more flexibility, as any single Memory subclass can be used to store the data. These are created when a new keyword argument is seen, and it does so by getting the data from a _Transformer input and setting its result as the source of a new Memory instance. The Memory type and initialization arguments are all specified in the LazyRunner initialization. 

KEEP IN MIND that these could mean a significant memory burden, if you are widly saving data from different inputs with several kwargs combinations passed on to them.

The solution to the memory problem comes in the buffer= initialization argument of the LazyRunner. These will limmit the number of Memory instances that are saved at any point. This also comes with the update= initialization argument for whether or not stored Memory instances should be updaed in FIFO order once the buffer is full or whether an error should be thrown.

KEEP IN MIND that this will not notice if its source data input has any sort of input changes itself (this could be a change in date range, for example or data source.) This will become a problem as changes will not be relayed if the runtime kwargs are the same as before a change. This happens as the LazyRunner will assume that nothing changed, see the kwarg and return the (old) saved version of the response. This can be solved by calling the .clear() method to reset the memory dictionary.

Keyword Arguments
-----------------


Tips and Tricks
===============

The Basic Workflow 
-------------------

When Reading the Docs
---------------------

Must-Know Classes
-----------------

